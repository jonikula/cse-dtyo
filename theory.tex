This chapter outlines the key issues of software testing as well as previous testing methods and their shortcomings.
After identifying these key issues and problems, the solutions offered by adopting model-based testing are presented.
The model-based testing process is then studied further. The steps from model generation to analysis of results are presented 
before focusing more on the key part of the testing process: the model. The text in this chapter is based largely on The Guide to the Software Engineering Body of Knowledge \cite{swebok} and Practical Model-Based Testing: A Tools Approach. \cite{tools}

\section{Software testing}
The Guide to the Software Engineering Body of Knowledge defines software testing as the dynamic verification that a program provides expected behaviors on a finite set of test cases, suitably selected from the usually infinite execution domain. From this definition, the authors extracted four words that describe the software testing knowledge area: dynamic, finite, selected and expected.

The authors use of the word dynamic to describe testing excludes the static analysis of the program from the field of testing, although it is described as complementary and different to dynamic testing. To be precise, the term dynamic is used to highlight the fact that even though testing always implies executing the program with selected inputs, the input values alone are not always sufficient to specify a test. This is due to the fact that complex, nondeterministic systems might react differently to the same input, depending on the state the software and the environment.
Actually executing the program during testing in a real or simulated environment is a significant advantage. The compiler, the libraries, the operating system and many other parts of the solution are tested alongside the correctness of the software design and code.

Testing is finite by necessity. Even in simple programs, so many possible test cases exist that covering them all could require more time and resources than is practical. For this reason, testing is conducted on a subset of tests, determined by risk and prioritization criteria. This also highlights the issue of test selection.

Test selection is a complex problem. It is also one of the key challenges in testing. There are many proposed test techinques and software engineers need to be aware of their effectiveness in a given situation. There are several informal strategies, such as equivalence class and boundary value testing that can help in determining the tests that are the most likely of being effective, and some of these techniques can also be applied in model-based testing.

The final key word, expected, is related to the oracle problem. After test execution, the result need to be analysed and a decision on the correctness of the behaviour needs to be made. The observed behaviour can be checked against user needs, specifications, anticipated behaviour and other kinds of requirements and expectations. Model-based testing can also be applied to generation of oracles as well as test inputs.

\section{Definition of model based testing}
The different kinds of testing can be described using a three-axis model presented by Tretmans \cite{tretmans}.  This model can be used to describe model-based testing as well. In the model, the axis represent the scale of the tested component, the source the tests are derived from and the characteristics being tested. The scale of the tested component runs all the way from software units, such as functions and classes, to individual components of software, integration testing and system testing. The second axis, characteristics, contains functional testing, robustness testing, performance testing and usability testing. The third axis is split into two by black-box and white-box testing. Using this model, model-based testing can be described as functional black-box testing and it can be applied to the entire SUT scale. There are some applications of model-based testing in performance and robustness testing as well, but especially performance testing methods for model-based testing are still under development.


The following approaches are the ones usually known as model-based testing.
 
\begin{enumerate}
	\item Generation of test input data from a domain model
	\item Generation of test cases from an environment model
	\item Generation of test cases with oracles from a behavior model
	\item Generation of test scripts from abstract tests
\end{enumerate}

Each of these can be said to be model-based testing, but some of them are not quite suitable when model-based testing is viewed in the context of tool development. In this context, the third definition is the one that draws the focus.
The first definition is an integral part of model-based testing, but it does not solve the entire problem of test design because it does not provide information of whether a test has passed or failed. Similarly to the first, the second definition does not provide information on the success or failure of the test case in any useful capacity, since it does not model the behaviour of the SUT. Rather it focuses on the environment around the SUT. The fourth definition is very different from the other three definitions. It assumes that an abstract description of a test case exists, perhaps in the form of an UML sequence diagram. This description is then transformed into an executable test script.

The third definition, the generation of test cases with oracles from a behavior model, is the one that gives us the most useful description of model-based testing. With this definition, the complete testing process can be automated, given a suitable model, and it produces test sequences that are suitable for transformation into executable test scripts.

With this view of model-based testing, Utting and Legeard define model-based testing as automation of the design of black-box tests.

\section{Testing methods and key issues}
As established earlier, model-based testing is usually applied to functional testing. For functional testing, Legeard and Utting present the following three key issues:

\begin{enumerate}
	\item Design of test cases
	\item Test execution and analysis of results
	\item Verifying how the tests cover the requirements
\end{enumerate}

There are several classic testing processes that are widely used and that try to address these issues. These processes are manual testing, capture/replay testing, script based testing and keyword-driven automated testing.

\subsection{Previous testing processes}
During the evolution of software engineering, testing has also been performed using a variety of techniques and processes. These processes and the ways they solve the key issues presented previously are discussed next, starting with the most basic process, the manual testing process.

An entirely manual testing process is still widely used in the industry, even though it is the earliest and simplest for of testing. In manual testing, the test designer creates a set of test cases based on the requirements documentation and the objectives, strategies and other information presented in a test plan. Test design is entirely manual and it's output is a human-readable document that describes each test case. The level of precision in these kinds of test case descriptions can be very low, as the low-level details regarding the SUT interaction can be left to the common sense of the tester. This type of test case design is very time consuming and it generally does not ensure systematic coverage of the functionality.

\begin{figure}[ht]
	\begin{center}
		\includegraphics*[width=0.3\textwidth]{kuvat/Manual_process}
	\end{center}
	\caption{Manual testing process}
	\label{fig:manualprocess}
\end{figure}

Test execution is also manual. The manual tester executes each test case by going through the steps of that test case, interacting directly with the SUT or with a test execution environment and analysing the output before recording the verdict for that test case.

This process is repeated every time a new release needs to be tested, and as is apparent from the process, it can be quite time consuming and resource intensive. The cost of testing each release is directly related to the size of the test suite and the cost of testing is constant as long as the size of the suite remains the same. This often results in the need to reduce the number of test cases being run as the software evolves, which introduces significant risks regarding product maturity, stability and robustness.

Capture/replay testing is a way to add automation to the manual testing process. The initial steps for this process are similar to that of the manual testing process. The test engineer creates a set of test cases, which he then executes manually. While executing the tests, the engineer uses a tool to capture the steps he performs on the software. These captured steps can then be automatically executed, removing the need to manually re-execute tests that have been previously captured.

\begin{figure}[ht]
	\begin{center}
		\includegraphics*[width=0.3\textwidth]{kuvat/CR_process}
	\end{center}
	\caption{Capture/replay testing process}
	\label{fig:crprocess}
\end{figure}

The idea here is solid, but it suffers similar problems as the manual testing process. It still initially requires lots of manual work to design and execute the test cases. The tests need to be re-executed manually every time the tested system changes as the captured test might no longer work on the new version of the software. In this case the tests need to be manually redone and captured again to get the automation to work with the new software.

A further improvement in terms of test automation is a process that relies on test scripts. These scripts are used to execute the test on the SUT and they can be automatically triggered for execution on different kinds of triggers, such as time or a commit done on a version control system. In this testing process the test engineer creates test scripts for each of the test cases using a programming or scripting language of some sort and adds them to the test suite. These tests are then automatically executed when necessary. This testing process requires very little manual testing after the test scripts have been created. Some manual testing may be done while developing the test scripts to verify functionality and to help in the development of the scripts.

After the test scripts have been created, only minimal maintenance work needs to be done to make sure the scripts are up to date with the current version of the tested software. Manual work is still needed when initially creating the test scripts but after the initial investment in time the maintenance cost is significantly reduced. These test scripts also usually include some sort of method to determine the verdict of the test, which makes it easy to automatically generate reports, graphs and other kinds of data. This reduces the time required for analyzing the test results, as all the relevant information can be presented to the tester in a format, which enables him to quickly determine which tests passed without issues, which tests failed for known or expected reasons and which tests require further analysis.

\begin{figure}[ht]
	\begin{center}
		\includegraphics*[width=0.3\textwidth]{kuvat/TS_process}
	\end{center}
	\caption{Script-based testing process}
	\label{fig:tsprocess}
\end{figure}

The next step forwards from using test scripts is to raise the level of abstraction of these scripts. Since the most significant maintenance cost for the script-based process is updating the test scripts to match the changes in SUT implementation, the natural solution to reducing it would be to implement some sort of adaptors or helper libraries, which could then be used across multiple test scripts, thus reducing the time spent on maintenance. This idea is implemented in the keyword-driven automated testing process. The idea in this process is to express the test cases in a very abstract manner, which is also precise enough to allow execution using test execution tools. The first form of this type of process is called data-driven testing, where a fixed set of test scripts is used with unique, parameterized data values for each test case. This makes the test scripts very generic so they can be used in multiple test cases.

Keyword-driven testing (also called action-word testing) goes a bit further by using action keywords in test cases. Each keyword corresponds to a fragment of code, which allows the test execution tool to translate these test cases into executable test scripts. This has the added benefit of reducing the level of programming knowledge required from the test designers.

\begin{figure}[ht]
	\begin{center}
		\includegraphics*[width=0.3\textwidth]{kuvat/KW_process}
	\end{center}
	\caption{Keyword-driven automated testing process}
	\label{fig:kwprocess}
\end{figure}

\subsection{Solved and remaining problems}
The manual testing process has a lot of problems. The resulting coverage of SUT functionality is often imprecise. It has no capabilities for regression testing and all testing is very time consuming and costly in manpower. It also provides no effective measurements of test coverage. The capture/replay process suffers from very similar problems, although it offers limited capabilities for automated regression testing.

Script based testing makes it possible to automatically execute and re-execute test cases. It is a significant evolution from the previous two processes, but this process too suffers from imprecise coverage of SUT functionality. It also comes with a set of it's own issues. Complex test scripts can be difficult to maintain and the requirements traceability needs to be developed manually. Keyword-driven testing eliminates the problems with test script maintenance but the remaining two issues are still valid for this process as well.

The model based testing process aims to solve the following remaining problems: Automation of test design, including generation of expected results, reduction of test suite maintenance costs and automatic generation of the traceability matrix from requirements to test cases.

\section{Model based testing process}
In a model-based testing process the idea is to take the informal model of the SUT behaviour and create a formal model that enables automatic creation of test cases. This would enable the automation of the design of test cases and generation of the traceability matrix. Instead of manually writing hundreds of test cases, the designer would write an abstract model, which the testing tool can use to generate a set of test cases. This would reduce the time spent on test design as well as allow generation of a variety of test suites by simply using different selection criteria.

The model-based testing process can be split into five main steps: Modelling of SUT and/or it's environment, generation of abstract tests, conversion of abstract tests into executable tests, test execution and verdicts and analysis of the results. The first two steps are the ones that distinguish model-based testing from the other processes described earlier.


\subsection{The model}
The first part of the testing process is the design of the abstract model. As the name suggests, this model should be a simplified version of the software and/or the environment it models. Instead of trying to model all interactions and features the model should be based on the key aspects that it is used to test. All unnecessary details should be omitted. Requirements identifiers should also be included in the model to document the relations between informal requirements and the formal model.

The model can be developed from scratch based on informal requirements. Some modeling of the system is already often done during the software design process and reusing those models could save a lot of time. The problem with this is that these models might not contain enough details to be useful for model based testing. There is usually a good middle ground, where high-level diagrams and use cases from development models are used for modeling the system for testing purposes and necessary behavioural details are added as needed. In any case, significant engineering effort is expected to be spent in modeling the system. Time spent in developing a good model pays off in lower maintenance costs later on, as the requirements and the software evolves. 

	
\subsubsection{Model design process}
The first step in model development is selecting the level of abstraction for the model. The amount of models required should also be selected early on. Smaller models describing subsystems can be more useful than a large model describing the entire system. UML class diagrams and other similar documentation generated for the software design process can be a good starting point for both of these initial steps. After selecting which parts of the system to model, the next step is to identify the data, operations and subsystems of the SUT that should be considered in the model. In all these areas, UML class diagrams and uses cases produced in the software design process can be very useful in all these initial steps.

After selecting the abstraction level and the contents of the model, the designer should select a notation to use and then write the model using that notation. The notation is often influenced by the tools that are available. In OSMO mbt tool \cite{OSMO}, for example, the model is written using the Java Programming Language. 

After writing the model, the next step is to ensure that the model is accurate. The model must specify the behaviour of the SUT and it should be correctly typed and consistent. Many tools suites offer some sort of a tool for simulating the behaviour of the model as well as syntax, types and many other properties of the model.

The final step is to generate tests from the model. In addition to providing feedback for the developers working on the SUT, these tests will also help the model designers and developers improve their model by pointing out issues in the initial documentation and requirements that translated into errors in the model as well as areas where the model might be overly complex or where it might not provide enough detail.  

Important things to keep in mind during the design process are the test objectives and the selected abstraction level. The model does not need to copy the SUT operations. Model operations can correspont to a sequence of operations in the SUT or even handle a subset of a single more complex SUT operation if necessary. 

\subsection{Advantages and disadvantages}
Model based testing is an evolution on the previous testing processes, designed to address some issues present in other processes. This process does require a different skillset from the testers used to manual testing and earlier forms of automated testing with scripts, emphasizing modeling and programming skills even more than scripting. Testers also need experience in recognising where modeling SUT may be difficult and where manually designed test cases might be a better approach compared to trying to work complex, very specific test scenarios through the automatic test generation. These can be significant disadvantages for organizations wanting to adopt model based testing. After the initial period of higher investment significant savings can be achieved when test maintenance and test suite size can be controlled by adjusting the model and generating a new suite. New features can be implemented into the model and added into the test suite generation without having to spend time manually implementing new tests. The process does also promote maturity in both the SUT design and the testing process. 

In examples provided by Utting and Legeard \cite{tools}, the performance in finding issues in implementation is mentioned as being similar or better than in previous processes. Model implementation is also said to be an excellent tool for finding issues in SUT requirements. Model based testing can also provide an easy way to improve certain types of coverage, as it is possible to easily generate hundreds of test sequences with either random or selected input variables. Traceability from test cases to requirements can also be implemented via the model, with requirements tested by each test case added into the correct checkpoints in the model.

\section{Test selection criteria}
-- Means of communicating choice of tests to tool

-- Not necessary to have knowledge of SUT code to generate tests: Instead based on model and requirements.

-- Model vs. SUT coverage criteria are complementary, use both!

-- Coverage criterion can be used to measure suiote adequacy or to define a stopping point for test generation.

-- Also prescriptive from tool point of view: "Try to achieve X coverage."

-- Families: Structural model coverage, data coverage, fault-model, requirements-based, explicit, statistical

\subsection{Structural model coverage}
-- Major issue in MBT is to measure and maximize coverage of model

-- Control-flow oriented, data-flow oriented, transition-based, UML-based

\subsubsection{Control-flow oriented coverage criterion}
--  Derived from classical code coverage criteria.

-- Statement-, decision-, and path coverage, where statement coverage is the weakest and path coverage the strongest.

-- Condidion coverage, decision/condition coverage, full predicate coverage, modified cond./dec. coverage, multiple condition coverage.

\subsubsection{Transition-based coverage criterion}
-- Useful for SUT:s and models that can be represented as FSMs or EFSMs (or LTS).

-- Transition based models made up of transitions and states.

-- All-transitions coverage, all-states coverage, all-configurations coverage, all-transition-pair coverage, all loop-free paths, all one-loop paths, all round-trips, all-paths.

-- Configuration: Current state of a parallel statechart.

-- All round-trips more usually more useful than all one-loop paths. It is weaker, but gives a linear number of tests (all one-loop paths usually exponential). TÄHÄN LÄHDE Testing object oriented systems: ... R.V. Binder.

-- Usually a good minimum requirement for FSM type models is to produce suite with all-transitions coverage requirement.

\subsection{Data-flow oriented coverage}
-- Working with definitions and uses of data variables

-- All-definitions, all definition/use paths, all use paths.