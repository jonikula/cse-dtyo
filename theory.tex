This chapter outlines the key issues of software testing as well as previous testing methods and their shortcomings.
After identifying these key issues and problems, the solutions offered by adopting model-based testing are presented.
The model-based testing process is then studied further. The steps from model generation to analysis of results are presented 
before focusing more on the key part of the testing process: the model. The text in this chapter is based largely on The Guide to the Software Engineering Body of Knowledge \cite{swebok} and Practical Model-Based Testing: A Tools Approach. \cite{tools}

\section{Software testing}
The Guide to the Software Engineering Body of Knowledge defines software testing as the dynamic verification that a program provides expected behaviors on a finite set of test cases, suitably selected from the usually infinite execution domain. From this definition, the authors extracted four words that describe the software testing knowledge area: dynamic, finite, selected and expected.

The authors use of the word dynamic to describe testing excludes the static analysis of the program from the field of testing, although it is described as complementary and different to dynamic testing. To be precise, the term dynamic is used to highlight the fact that even though testing always implies executing the program with selected inputs, the input values alone are not always sufficient to specify a test. This is due to the fact that complex, nondeterministic systems might react differently to the same input, depending on the state the software and the environment.
Actually executing the program during testing in a real or simulated environment is a significant advantage. The compiler, the libraries, the operating system and many other parts of the solution are tested alongside the correctness of the software design and code.

Testing is finite by necessity. Even in simple programs, so many possible test cases exist that covering them all could require more time and resources than is practical. For this reason, testing is conducted on a subset of tests, determined by risk and prioritization criteria. This also highlights the issue of test selection.

Test selection is a complex problem. It is also one of the key challenges in testing. There are many proposed test techinques and software engineers need to be aware of their effectiveness in a given situation. There are several informal strategies, such as equivalence class and boundary value testing that can help in determining the tests that are the most likely of being effective, and some of these techniques can also be applied in model-based testing.

The final key word, expected, is related to the oracle problem. After test execution, the result need to be analysed and a decision on the correctness of the behaviour needs to be made. The observed behaviour can be checked against user needs, specifications, anticipated behaviour and other kinds of requirements and expectations. Model-based testing can also be applied to generation of oracles as well as test inputs.

\section{Definition of model based testing}
The different kinds of testing can be described using a three-axis model presented by Tretmans \cite{tretmans}.  This model can be used to describe model-based testing as well. In the model, the axis represent the scale of the tested component, the source the tests are derived from and the characteristics being tested. The scale of the tested component runs all the way from software units, such as functions and classes, to individual components of software, integration testing and system testing. The second axis, characteristics, contains functional testing, robustness testing, performance testing and usability testing. The third axis is split into two by black-box and white-box testing. Using this model, model-based testing can be described as functional black-box testing and it can be applied to the entire SUT scale. There are some applications of model-based testing in performance and robustness testing as well, but especially performance testing methods for model-based testing are still under development.


The following approaches are the ones usually known as model-based testing.
 
\begin{enumerate}
	\item Generation of test input data from a domain model
	\item Generation of test cases from an environment model
	\item Generation of test cases with oracles from a behavior model
	\item Generation of test scripts from abstract tests
\end{enumerate}

Each of these can be said to be model-based testing, but some of them are not quite suitable when model-based testing is viewed in the context of tool development. In this context, the third definition is the one that draws the focus.
The first definition is an integral part of model-based testing, but it does not solve the entire problem of test design because it does not provide information of whether a test has passed or failed. Similarly to the first, the second definition does not provide information on the success or failure of the test case in any useful capacity, since it does not model the behaviour of the SUT. Rather it focuses on the environment around the SUT. The fourth definition is very different from the other three definitions. It assumes that an abstract description of a test case exists, perhaps in the form of an UML sequence diagram. This description is then transformed into an executable test script.

The third definition, the generation of test cases with oracles from a behavior model, is the one that gives us the most useful description of model-based testing. With this definition, the complete testing process can be automated, given a suitable model, and it produces test sequences that are suitable for transformation into executable test scripts.

With this view of model-based testing, Utting and Legeard define model-based testing as automation of the design of black-box tests.

\section{Testing methods and key issues}
As established earlier, model-based testing is usually applied to functional testing. For functional testing, Legeard and Utting present the following three key issues:

\begin{enumerate}
	\item Design of test cases
	\item Test execution and analysis of results
	\item Verifying how the tests cover the requirements
\end{enumerate}

There are several classic testing processes that are widely used and that try to address these issues. These processes are manual testing, capture/replay testing, script based testing and keyword-driven automated testing.

\subsection{A Manual Testing Process}
An entirely manual testing process is still widely used in the industry, even though it is the earliest and simplest for of testing. In manual testing, the test designer creates a set of test cases based on the requirements documentation and the objectives, strategies and other information presented in a test plan. Test design is entirely manual and it's output is a human-readable document that describes each test case. The level of precision in these kinds of test case descriptions can be very low, as the low-level details regarding the SUT interaction can be left to the common sense of the tester. This type of test case design is very time consuming and it generally does not ensure systematic coverage of the functionality.

Here test execution is also manual. The manual tester executes each test case by going through the steps of that test case, interacting directly with the SUT or with a test execution environment and analysing the output before recording the verdict for that test case.

This process is repeated every time a new release needs to be tested, and as is apparent from the process, it can be quite time consuming and resource intensive. The cost of testing each release is directly related to the size of the test suite and the cost of testing is constant as long as the size of the suite remains the same. This often results in the need to reduce the number of test cases being run as the software evolves, which introduces significant risks regarding product maturity, stability and robustness.

\subsection{Capture/replay Testing Process}
Capture/replay testing is a way to add automation to the manual testing process. The initial steps for this process are similar to that of the manual testing process. The test engineer creates a set of test cases, which he then executes manually. While executing the tests, the engineer uses a tool to capture the steps he performs on the software. These captured steps can then be automatically executed, removing the need to manually re-execute tests that have been previously captured.

The idea here is solid, but it suffers similar problems as the manual testing process. It still initially requires lots of manual work to execute the test cases. The captured tests need to also be re-executed manually every time the tested system changes as the captured test might no longer work on the new version of the software. In this case the tests need to be manually redone and captured again to get the automation to work with the new software.

\subsection{Script-based Testing Process}
A better form of automating testing is based on test scripts. These scripts are used to execute the test on the SUT and they can be automatically triggered for execution on different kinds of triggers, such as time or a commit done on a version control system. In this testing process the test engineer creates test scripts for each of the test cases using a programming or scripting language of some sort and adds them to the test suite. These tests are then automatically executed when necessary. This testing process requires very little manual testing after the test scripts have been created. Some manual testing may be done while developing the test scripts to verify functionality and to help in the development of the scripts.

After the test scripts have been created, only minimal maintenance work needs to be done to make sure the scripts are up to date with the current version of the tested software. Manual work is still needed when initially creating the test scripts but after the initial investment in time the maintenance cost is significantly reduced.


-- Solved and remaining problems

\section{Model based testing process}
TÄHÄN LISÄTÄÄN KUVA

-- Main idea is to take the informal, mental model of expected SUT behaviour and create a formal model that enables automatic tc creation. 

-- Five main steps
	-- Model SUT and/or it's environment
	-- Generate abstract tests
	-- Concretize abs. tests into executable tests
	-- Execute tests and assign verdicts
	-- Analyze results

\subsection{The model}
-- Must be concice and precise. 

-- Most essential thing in MBT.

-- Problem: Using development models or develop entirely own models for testing? 
	-- usually a good middle ground, reuse of high-level diagrams and some use cases from dev. models and adding behavioural details nexessary for MBT.

-- Work must be spent on developing initial model, but pays off in lower maintenance costs.
	-- requirements evolution handled by evolving model, new tc:s then generated automatically from model! 
	
\subsubsection{Model design process}
-- First decide on level of abstraction

-- CD of SUT parts relevant to model is a good starting point.

-- Useful to create smaller models for subsystems instead of large, complex models for the entire system. 

-- Decide on which parts to model

-- identify data and operations required for modeling, with primary focus on SUT.
	-- Omit everything that is unnecessary for the model and include only the operations you wish to test.
	
-- Design model to meet test objectives!

-- Model operations do not need to copy SUT operations, can instead correspond to a sequence of operations or a subset of a single more complex SUT operation!

-- Select notation

\subsection{Advantages and disadvantages}

\subsubsection{Advantages}
-- Saves resources after initial training period when compared to other methods

-- Similar or better performance in finding issues in implementation

-- Lots of issues found in requirements due to precision required in model generation

-- Improved coverage

-- Promotes maturity in SUT development process

-- Traceability from test cases to requirements

\subsubsection{Disadvantages}
-- Requires reasonable maturity from SUT development process and testing processes, as model generation requires proper requirements for SUT and well made models in requirements help reduce time required to generate models for testing.

-- Also requires experience with automated test execution

-- Requires different skills from testers: Modeling skills from test designers, programming skills. Implies some training costs and an initial learning curve.

-- Pain factors after adopting MBT: Outdated requirements cause lots of false errors. Eperience required from testers to recognice where modeling SUT may be difficult and where manually designed test cases might be a better approach. Generated test cases might not always be intuitive to analyze compared to manually created ones. Metrics like "number of test cases run" become useless as it is trivial to generate enormous test suites.

\section{Test selection criteria}
-- Means of communicating choice of tests to tool

-- Not necessary to have knowledge of SUT code to generate tests: Instead based on model and requirements.

-- Model vs. SUT coverage criteria are complementary, use both!

-- Coverage criterion can be used to measure suiote adequacy or to define a stopping point for test generation.

-- Also prescriptive from tool point of view: "Try to achieve X coverage."

-- Families: Structural model coverage, data coverage, fault-model, requirements-based, explicit, statistical

\subsection{Structural model coverage}
-- Major issue in MBT is to measure and maximize coverage of model

-- Control-flow oriented, data-flow oriented, transition-based, UML-based

\subsubsection{Control-flow oriented coverage criterion}
--  Derived from classical code coverage criteria.

-- Statement-, decision-, and path coverage, where statement coverage is the weakest and path coverage the strongest.

-- Condidion coverage, decision/condition coverage, full predicate coverage, modified cond./dec. coverage, multiple condition coverage.

\subsubsection{Transition-based coverage criterion}
-- Useful for SUT:s and models that can be represented as FSMs or EFSMs (or LTS).

-- Transition based models made up of transitions and states.

-- All-transitions coverage, all-states coverage, all-configurations coverage, all-transition-pair coverage, all loop-free paths, all one-loop paths, all round-trips, all-paths.

-- Configuration: Current state of a parallel statechart.

-- All round-trips more usually more useful than all one-loop paths. It is weaker, but gives a linear number of tests (all one-loop paths usually exponential). TÄHÄN LÄHDE Testing object oriented systems: ... R.V. Binder.

-- Usually a good minimum requirement for FSM type models is to produce suite with all-transitions coverage requirement.

\subsection{Data-flow oriented coverage}
-- Working with definitions and uses of data variables

-- All-definitions, all definition/use paths, all use paths.